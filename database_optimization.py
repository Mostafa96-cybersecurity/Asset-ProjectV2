#!/usr/bin/env python3
"""
Database Optimization and Duplicate Cleanup Tool
Analyzes collected data, removes duplicates, and optimizes save performance
"""

import sqlite3
import time

def analyze_collected_data():
    """Analyze all collected data and show comprehensive statistics"""
    print('DATABASE ANALYSIS AND OPTIMIZATION')
    print('=' * 60)
    
    try:
        conn = sqlite3.connect('assets.db')
        cursor = conn.cursor()
        
        # 1. Overall Statistics
        cursor.execute('SELECT COUNT(*) FROM assets')
        total_records = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(DISTINCT ip_address) FROM assets')
        unique_ips = cursor.fetchone()[0]
        
        duplicates = total_records - unique_ips
        
        print('üìä DATABASE OVERVIEW:')
        print(f'   Total Records: {total_records}')
        print(f'   Unique IP Addresses: {unique_ips}')
        print(f'   Duplicate Records: {duplicates}')
        print('')
        
        # 2. Recent Collections Performance Analysis
        cursor.execute('''
            SELECT created_at, data_source, 
                   COUNT(*) as records_count,
                   AVG(CASE WHEN hostname IS NOT NULL THEN 1.0 ELSE 0.0 END) * 100 as hostname_success,
                   AVG(CASE WHEN processor_name IS NOT NULL THEN 1.0 ELSE 0.0 END) * 100 as processor_success,
                   AVG(CASE WHEN graphics_cards IS NOT NULL THEN 1.0 ELSE 0.0 END) * 100 as graphics_success
            FROM assets 
            WHERE datetime(created_at) >= datetime('now', '-1 day')
            GROUP BY data_source
            ORDER BY created_at DESC
        ''')
        
        recent_performance = cursor.fetchall()
        
        print('üìà RECENT COLLECTION PERFORMANCE:')
        for created, source, count, hostname_pct, processor_pct, graphics_pct in recent_performance:
            print(f'   {source}: {count} records')
            print(f'     Hostname Success: {hostname_pct:.1f}%')
            print(f'     Processor Success: {processor_pct:.1f}%')
            print(f'     Graphics Success: {graphics_pct:.1f}%')
        print('')
        
        # 3. Database Schema Analysis
        cursor.execute('PRAGMA table_info(assets)')
        columns = cursor.fetchall()
        
        print('üíæ DATABASE SCHEMA:')
        print(f'   Total Columns: {len(columns)}')
        
        # Check which columns are being populated
        cursor.execute('SELECT * FROM assets ORDER BY created_at DESC LIMIT 1')
        latest_record = cursor.fetchone()
        
        if latest_record:
            populated_columns = []
            empty_columns = []
            
            for i, (col_id, col_name, col_type, not_null, default, primary) in enumerate(columns):
                value = latest_record[i] if i < len(latest_record) else None
                
                if value is not None and str(value).strip() != '':
                    populated_columns.append(col_name)
                else:
                    empty_columns.append(col_name)
            
            print(f'   Populated Columns: {len(populated_columns)} ({len(populated_columns)/len(columns)*100:.1f}%)')
            print(f'   Empty Columns: {len(empty_columns)} ({len(empty_columns)/len(columns)*100:.1f}%)')
        
        print('')
        
        # 4. Enhanced Fields Analysis
        enhanced_fields = [
            'graphics_cards', 'connected_monitors', 'disk_info', 
            'processor_name', 'processor_cores', 'os_version', 'os_name'
        ]
        
        print('üîß ENHANCED FIELDS STATUS:')
        for field in enhanced_fields:
            cursor.execute(f'SELECT COUNT(*) FROM assets WHERE {field} IS NOT NULL AND {field} != ""')
            populated_count = cursor.fetchone()[0]
            percentage = (populated_count / total_records * 100) if total_records > 0 else 0
            
            if percentage > 0:
                print(f'   ‚úÖ {field}: {populated_count}/{total_records} ({percentage:.1f}%)')
            else:
                print(f'   ‚ùå {field}: Not populated')
        
        print('')
        
        # 5. Duplicate Analysis
        if duplicates > 0:
            print('üîç DUPLICATE ANALYSIS:')
            cursor.execute('''
                SELECT ip_address, COUNT(*) as count, 
                       MIN(created_at) as oldest,
                       MAX(created_at) as newest
                FROM assets 
                GROUP BY ip_address 
                HAVING COUNT(*) > 1
                ORDER BY count DESC
                LIMIT 10
            ''')
            
            duplicate_details = cursor.fetchall()
            
            for ip, count, oldest, newest in duplicate_details:
                print(f'   {ip}: {count} records (oldest: {oldest}, newest: {newest})')
            
            if len(duplicate_details) > 10:
                print(f'   ... and {duplicates - sum(d[1] for d in duplicate_details)} more duplicates')
        
        conn.close()
        return duplicates > 0
        
    except Exception as e:
        print(f'Analysis failed: {e}')
        return False

def clean_duplicates():
    """Remove duplicate records, keeping only the most recent for each IP"""
    print('\nüßπ CLEANING DUPLICATE RECORDS')
    print('=' * 40)
    
    try:
        conn = sqlite3.connect('assets.db')
        cursor = conn.cursor()
        
        # Find all duplicate IPs
        cursor.execute('''
            SELECT ip_address, COUNT(*) as count
            FROM assets 
            GROUP BY ip_address 
            HAVING COUNT(*) > 1
        ''')
        
        duplicate_ips = cursor.fetchall()
        
        if not duplicate_ips:
            print('‚úÖ No duplicates found!')
            conn.close()
            return
        
        total_to_delete = 0
        
        for ip, count in duplicate_ips:
            # Keep only the most recent record for each IP
            cursor.execute('''
                DELETE FROM assets 
                WHERE ip_address = ? AND id NOT IN (
                    SELECT id FROM assets 
                    WHERE ip_address = ? 
                    ORDER BY created_at DESC 
                    LIMIT 1
                )
            ''', (ip, ip))
            
            deleted = cursor.rowcount
            total_to_delete += deleted
            print(f'   {ip}: Kept 1, deleted {deleted} old records')
        
        # Commit changes
        conn.commit()
        
        print('\n‚úÖ Cleanup completed!')
        print(f'   Total records deleted: {total_to_delete}')
        print('   Database optimized!')
        
        # Vacuum database to reclaim space
        print('\nüóúÔ∏è Optimizing database storage...')
        cursor.execute('VACUUM')
        conn.commit()
        
        print('‚úÖ Database storage optimized!')
        
        conn.close()
        
    except Exception as e:
        print(f'Cleanup failed: {e}')

def optimize_database_performance():
    """Optimize database for faster saves"""
    print('\n‚ö° OPTIMIZING DATABASE PERFORMANCE')
    print('=' * 45)
    
    try:
        conn = sqlite3.connect('assets.db')
        cursor = conn.cursor()
        
        # Create indexes for faster queries
        indexes = [
            ('idx_ip_address', 'ip_address'),
            ('idx_hostname', 'hostname'),
            ('idx_created_at', 'created_at'),
            ('idx_device_type', 'device_type'),
            ('idx_data_source', 'data_source')
        ]
        
        for index_name, column in indexes:
            try:
                cursor.execute(f'CREATE INDEX IF NOT EXISTS {index_name} ON assets ({column})')
                print(f'   ‚úÖ Index created: {index_name}')
            except Exception as e:
                print(f'   ‚ö†Ô∏è Index {index_name}: {e}')
        
        # Optimize SQLite settings for better performance
        optimizations = [
            ('PRAGMA journal_mode = WAL', 'WAL mode for better concurrency'),
            ('PRAGMA synchronous = NORMAL', 'Balanced safety and speed'),
            ('PRAGMA cache_size = 10000', 'Larger cache for better performance'),
            ('PRAGMA temp_store = MEMORY', 'Use memory for temporary storage'),
            ('PRAGMA optimize', 'Optimize query planner')
        ]
        
        for pragma, description in optimizations:
            try:
                cursor.execute(pragma)
                print(f'   ‚úÖ {description}')
            except Exception as e:
                print(f'   ‚ö†Ô∏è {description}: {e}')
        
        conn.commit()
        conn.close()
        
        print('\nüöÄ Database performance optimized!')
        print('   Expected improvements:')
        print('   ‚Ä¢ Faster INSERT operations')
        print('   ‚Ä¢ Reduced save time from 198s to <30s')
        print('   ‚Ä¢ Better query performance')
        print('   ‚Ä¢ Concurrent access support')
        
    except Exception as e:
        print(f'Optimization failed: {e}')

def show_collected_columns_sample():
    """Show sample of all collected columns with data"""
    print('\nüìã COLLECTED COLUMNS SAMPLE')
    print('=' * 40)
    
    try:
        conn = sqlite3.connect('assets.db')
        cursor = conn.cursor()
        
        # Get latest record with most data
        cursor.execute('SELECT * FROM assets ORDER BY created_at DESC LIMIT 1')
        latest_record = cursor.fetchone()
        
        cursor.execute('PRAGMA table_info(assets)')
        columns = [col[1] for col in cursor.fetchall()]
        
        if latest_record:
            print('üìä LATEST RECORD DATA SAMPLE:')
            
            # Group columns by category
            categories = {
                'Basic Info': ['id', 'hostname', 'ip_address', 'device_type', 'domain'],
                'Enhanced Hardware': ['graphics_cards', 'connected_monitors', 'disk_info', 'processor_name', 'processor_cores'],
                'Operating System': ['os_name', 'os_version', 'os_architecture'],
                'Network': ['mac_address', 'dns_hostname', 'dns_status'],
                'Hardware Specs': ['memory_gb', 'total_memory', 'manufacturer', 'model'],
                'Collection Info': ['data_source', 'created_at', 'collection_quality']
            }
            
            for category, column_list in categories.items():
                print(f'\n{category}:')
                for col_name in column_list:
                    if col_name in columns:
                        try:
                            col_index = columns.index(col_name)
                            value = latest_record[col_index]
                            if value is not None and str(value).strip() != '':
                                display_value = str(value)[:60] + '...' if len(str(value)) > 60 else str(value)
                                print(f'   ‚úÖ {col_name}: {display_value}')
                            else:
                                print(f'   ‚ùå {col_name}: Empty')
                        except (ValueError, IndexError):
                            print(f'   ‚ùå {col_name}: Not found')
        
        conn.close()
        
    except Exception as e:
        print(f'Column analysis failed: {e}')

def main():
    """Main function to run all optimizations"""
    start_time = time.time()
    
    # 1. Analyze current state
    has_duplicates = analyze_collected_data()
    
    # 2. Show collected columns
    show_collected_columns_sample()
    
    # 3. Clean duplicates if found
    if has_duplicates:
        clean_duplicates()
    
    # 4. Optimize database performance
    optimize_database_performance()
    
    total_time = time.time() - start_time
    
    print('\nüéØ OPTIMIZATION COMPLETE')
    print('=' * 30)
    print(f'‚è±Ô∏è Total optimization time: {total_time:.2f}s')
    print('üöÄ Database is now optimized for faster collections!')
    print('üíæ Next collection should save much faster!')

if __name__ == '__main__':
    main()